# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2Ux5VKWZDyAU17tsvlFQ-ht74RMNma4
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchvision
from torch.utils.data import DataLoader, Dataset
import nltk
from nltk.tokenize import word_tokenize
from torchvision import transforms

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('punkt_tab')

from google.colab import files
uploaded = files.upload()

df=pd.read_csv('data.csv')

df.info()

df.head()

X=df['Sentence']
y=df['Sentiment']

X_numpy=X.to_numpy()
y_numpy=y.to_numpy()

y_encoded=pd.get_dummies(y,dtype=int)

y_encoded

y_tensor=torch.tensor(y_encoded.values)

y_tensor

corpus=X_numpy

nltk.download("stopwords")
nltk.download("wordnet")

from nltk.corpus import stopwords
stop_words=set(stopwords.words('english'))



from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def Lemmatize(corpus):
  new_corpus=[]
  for i in range(len(corpus)):
    words=nltk.word_tokenize(corpus[i])
    words=[lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]
    new_sentence=(' '.join(words))
    new_corpus.append(new_sentence)
  return new_corpus

lemmatized_corpus=Lemmatize(corpus)

lemmatized_corpus[2]

corpus[2]

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(lemmatized_corpus)
tokens=tokenizer.texts_to_sequences(lemmatized_corpus)

# Step 2: Convert to tensors
tokens = [torch.tensor(seq, dtype=torch.long) for seq in tokens]
lengths = torch.tensor([len(seq) for seq in tokens])

# Step 3: Sort by sequence length (required for packing)
lengths, sort_idx = lengths.sort(descending=True)
tokens = [tokens[i] for i in sort_idx]
labels = torch.tensor(y_tensor)[sort_idx]

# Step 4: Pad the tokens
padded = pad_sequence(tokens, batch_first=True, padding_value=0)

# Step 5: Pack the padded batch
packed = pack_padded_sequence(padded, lengths.cpu(), batch_first=True)

print(f"Padded shape: {padded.shape}")        # e.g., torch.Size([3, 3])
print(f"Packed batch data: {packed.data}")    # Packed sequence values
print(f"Packed batch batch_sizes: {packed.batch_sizes}")  # batch sizes per timestep

# Hyperparameters
input_size=lengths
embedded_dim=256
hidden_size=64
num_layers=4
num_classes=3
learning_rate=0.01
batch_size=1
num_epochs=2

class LSTMnetwork(nn.Module):
  def __init__(self,input_size,embedded_dim,hidden_size,num_layers,num_classes):
    super(LSTMnetwork,self).__init__()
    self.embedding=nn.Embedding(input_size,embedded_dim)
    self.lstm=nn.LSTM(embedded_dim,hidden_size,num_layers,batch_first=True)
    self.fc=nn.Linear(hidden_size,num_classes)

  def forward(self,x):
    out=self.embedding(x)
    out,_=self.lstm(out)
    out=out[:,-1,:]
    out=self.fc(out)
    return out

model=LSTMnetwork(input_size,embedded_dim,hidden_size,num_layers,num_classes)
k=model(packed)

